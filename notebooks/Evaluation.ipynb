{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddab6212-a2aa-4e2f-94d0-ae44d4261b11",
   "metadata": {},
   "source": [
    "# 0. Define evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de12483-f548-4b8f-8605-750c3057d3c6",
   "metadata": {},
   "source": [
    "Some evaluation metrics for information retrieval systems like this semantic search engine are:\n",
    "* __Mean Reciprocal Rank (MRR)__ is a ranking quality metric. It considers the position of the first relevant item in the ranked list.\n",
    "You can calculate MRR as the mean of Reciprocal Ranks across all users or queries. \n",
    "A Reciprocal Rank is the inverse of the position of the first relevant item. If the first relevant item is in position 2, the reciprocal rank is 1/2. \n",
    "* __Normalized Discounted Cumulative Gain (NDCG)__ is a ranking quality metric. It compares rankings to an ideal order where all relevant items are at the top of the list.\n",
    "NDCG at K is determined by dividing the Discounted Cumulative Gain (DCG) by the ideal DCG representing a perfect ranking. \n",
    "DCG measures the total item relevance in a list with a discount that helps address the diminishing value of items further down the list.\n",
    "* __Recall at K__ measures the proportion of correctly identified relevant items in the top K recommendations out of the total number of relevant items in the dataset. In simpler terms, it indicates how many of the relevant items you could successfully find.\n",
    "* __Precision at K__ is the ratio of correctly identified relevant items within the total recommended items inside the K-long list. Simply put, it shows how many recommended or retrieved items are genuinely relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c59ee3-0b4b-4461-9e32-f5f8fa67a0a7",
   "metadata": {},
   "source": [
    "As you can understand, __Recall at K__ metrics require the informantion of the total number of relevant items. In the dataset used in this repository, there is no such information, so we will avoid applying this metric on evaluating the system on the set of test queries.\n",
    "For the others metrics, we can easily label by hand the results and then compute the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927066b3-d93b-48f9-9634-b51f9cc0a5ed",
   "metadata": {},
   "source": [
    "# 1. Implement the code for communicating with the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15cdd55d-800b-4945-85e7-ef79820d9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from sklearn.metrics import precision_score, ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b48a6b50-dd7c-45a3-ab8e-5eb20445b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_ENDPOINT = \"http://0.0.0.0:5000/query\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f8c2272-f5b4-4968-bbd2-8c6980a0265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(api_endpoint: str,\n",
    "             vector_to_search: str,\n",
    "             k: int,\n",
    "             text_query: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    retrieve the most relevant images based on a text query\n",
    "    :param api_endpoint: the API endpoint to retreive the relevant images.\n",
    "    :param vector_to_search: if \"text\" it will retrieve the most relavant images based on the caption embeddings. If it is \"image\"\n",
    "    it will retrieve the most relevant images based on the image embedding\n",
    "    :param k: the number of the top-k most relevant retrieved images\n",
    "    :param text_query: the user's query\n",
    "    :return a list of the captions of the images. Based on them, I will decide if it is correctly retrieved(label 1) or not(label 0)\n",
    "    \"\"\"\n",
    "    response = requests.post(api_endpoint,\n",
    "                            data=json.dumps({\n",
    "                                \"text\": text_query,\n",
    "                                \"k\": k,\n",
    "                                \"vector_to_search\": vector_to_search\n",
    "                            })).json()\n",
    "    return [metadata[\"captions\"][0] for metadata in response]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b98e49-397b-4f64-b743-a6bbfcfded98",
   "metadata": {},
   "source": [
    "To make my life easier in labelling and evaluating this system, I will retrieve on the first caption/answer of each image. You can have a look at the following cell, how the function works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "249ccfa9-4295-49fa-8c79-d4a2762cd871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A cat in between two cars in a parking lot.',\n",
       " 'A cute kitten is sitting in a dish on a table.',\n",
       " 'A black cat is inside a white toilet.',\n",
       " 'a man sleeping with his cat next to him',\n",
       " 'A cat eating a bird it has caught.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve(api_endpoint=API_ENDPOINT,\n",
    "         k=5,\n",
    "         text_query=\"a cat playing alone\",\n",
    "         vector_to_search=\"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50bf213-7ba3-4813-a166-79e97f99073a",
   "metadata": {},
   "source": [
    "# 2. Evaluation process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08552cff-5d67-467e-92a6-83bfca0db62f",
   "metadata": {},
   "source": [
    "## 2.1 Define the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7559dc79-6d9a-4a9b-82bf-437d73f00522",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [\"a cat playing alone\",\n",
    "            \"photo of a car\",\n",
    "            \"photo of a dog\",\n",
    "            \"photo of a human with an animal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55acabe4-9dac-4bd8-a392-847358986135",
   "metadata": {},
   "source": [
    "## 2.2 Retrieve the most relevant images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9a22dfa-419c-4b98-8774-423c98313a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A cat in between two cars in a parking lot.', 'A cute kitten is sitting in a dish on a table.', 'A black cat is inside a white toilet.', 'a man sleeping with his cat next to him', 'A cat eating a bird it has caught.']\n",
      "['A small car is parked in front of a scooter', 'A car is stopped at a red light', 'Fog is in the air at an intersection with several traffic lights.', 'An old-fashioned green station wagon is parked on a shady driveway.', 'A cat in between two cars in a parking lot.']\n",
      "['A door with a sticker of a cat door on it', \"Two husky's hanging out of the car windows.\", 'A black cat is inside a white toilet.', \"A trio of dogs sitting in their owner's lap in a red convertible.\", 'A fireplace with a fire built in it.']\n",
      "['a man sleeping with his cat next to him', 'A man sits with a traditionally decorated cow', 'A shot of an elderly man inside a kitchen.', 'A black and white photo of an older man skiing.', 'A person holding a skateboard overlooks a dead field of crops.']\n"
     ]
    }
   ],
   "source": [
    "for query in test_set:\n",
    "    print(retrieve(api_endpoint=API_ENDPOINT,\n",
    "         k=5,\n",
    "         text_query=query,\n",
    "         vector_to_search=\"image\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f627c37-e7ce-446f-a55c-0d4b028fb407",
   "metadata": {},
   "source": [
    "There is no need to store the retrieved captions in a variable. However, we need to store the labels of the retrieved documents in variables. It will facilitate the computation of the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9981a8-9e43-4176-b2ad-661bc108f2cb",
   "metadata": {},
   "source": [
    "## 2.3 Labelling retrieved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db40232e-8ca2-44b7-9750-dda3dfa0e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [\n",
    "    [1, 1, 1, 1, 1], # each image has a cat, is labelled as 1.\n",
    "    [1, 1, 0, 1, 1], # each image displays a car, is labelled as 1.\n",
    "    [0, 1, 0, 1, 0], # each image shows a dog, is labelled as 1\n",
    "    [1, 1, 0, 0, 0] # each image shows an animal with a person is labelled as 1.\n",
    "] # the true labels of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58653a98-da1d-41d4-ae10-4b4cda14eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [[1, 1, 1, 1, 1],\n",
    "         [1, 1, 1, 1, 1],\n",
    "         [1, 1, 1, 1, 1],\n",
    "         [1, 1, 1, 1, 1]\n",
    "        ] # the labels the retrieval process returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a888d7a6-06e4-467c-bdec-70505128b84d",
   "metadata": {},
   "source": [
    "## 2.4 Evaluating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ccefaa36-cfaf-4f55-8afb-f2308444a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(y_preds:list[list[int]]) -> float:\n",
    "    \"\"\"\n",
    "    compute the mrr score\n",
    "    :param y_preds: the prediction labels\n",
    "    :return the mrr score\n",
    "    \"\"\"\n",
    "    mrr = 0\n",
    "    for y_pred in y_preds:\n",
    "        for index, pred in enumerate(y_pred):\n",
    "            if pred == 1:\n",
    "                mrr += 1/(index + 1)\n",
    "                break\n",
    "    mrr /= len(y_preds)\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46eb856e-352d-4c84-9189-cace2bea0930",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_precision_at_5 = precision_score(y_true=true_labels,\n",
    "                                       y_pred=preds,\n",
    "                                       average=\"micro\")\n",
    "ndcg_at_5 = ndcg_score(y_true=true_labels, y_score=preds)\n",
    "mrr_at_5 = mrr(y_preds=true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "182476eb-4487-4abf-87ea-e1e0cc8d9a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@5 is: 0.65\n",
      "NDCG@5 is: 0.8417718099904842\n",
      "MRR@5 is: 0.875\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision@5 is:\", micro_precision_at_5)\n",
    "print(\"NDCG@5 is:\", ndcg_at_5)\n",
    "print(\"MRR@5 is:\", mrr_at_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c06ad0-c321-494b-8ceb-24854163a550",
   "metadata": {},
   "source": [
    "## 3. Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3969555e-e0be-46c3-833d-7c5939b3bab5",
   "metadata": {},
   "source": [
    "An error I noticed is that the clip model is not able to understant the word \"without\". I will display some examples in the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "038105f7-f80d-4d1b-bcde-8c6a246bb4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_test_set = [\"Image of a bird without a cat\",\n",
    "                 \"Image of a people without an animal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e94bb4-8834-4f1e-8750-36e4f989d547",
   "metadata": {},
   "source": [
    "## 3.1 Retrieving the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4334bbd0-ab5d-432a-80ac-2b52ebc97671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A cat in between two cars in a parking lot.', 'A cat eating a bird it has caught.', 'A door with a sticker of a cat door on it', 'A black cat is inside a white toilet.', 'A cute kitten is sitting in a dish on a table.']\n",
      "['A man sits with a traditionally decorated cow', 'A man is sitting on a bench next to a bike.', 'A brown and black horse in the middle of the city eating grass.', 'A shot of an elderly man inside a kitchen.', 'A door with a sticker of a cat door on it']\n"
     ]
    }
   ],
   "source": [
    "for query in error_test_set:\n",
    "    print(retrieve(api_endpoint=API_ENDPOINT,\n",
    "         k=5,\n",
    "         text_query=query,\n",
    "         vector_to_search=\"image\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99a4c2-8339-4aa5-b2a6-10d8afe3fe41",
   "metadata": {},
   "source": [
    "## 3.2 Define labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1e447cc-363f-4595-8c1c-483146ef4faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [\n",
    "    [0, 0, 0, 0, 0], # each image has a bird without displaying a cat, is labelled as 1.\n",
    "    [0, 1, 0, 1, 0], # each image displays a human without displaying an animal, is labelled as 1.\n",
    "] # the true labels of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a51e6c3-c46f-4eb2-93b8-c2633b1fe568",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [[1, 1, 1, 1, 1],\n",
    "         [1, 1, 1, 1, 1]\n",
    "        ] # the labels the retrieval process returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d8641e-716a-4904-8de0-29f75effc0fc",
   "metadata": {},
   "source": [
    "## 3.3 Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe375bba-4e25-4e77-aa2e-9668d93bccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_precision_at_5 = precision_score(y_true=true_labels,\n",
    "                                       y_pred=preds,\n",
    "                                       average=\"micro\")\n",
    "ndcg_at_5 = ndcg_score(y_true=true_labels, y_score=preds)\n",
    "mrr_at_5 = mrr(y_preds=true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ce4fffa-2d37-4845-bb88-5652b922842e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@5 is: 0.2\n",
      "NDCG@5 is: 0.36156788634492326\n",
      "MRR@5 is: 0.25\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision@5 is:\", micro_precision_at_5)\n",
    "print(\"NDCG@5 is:\", ndcg_at_5)\n",
    "print(\"MRR@5 is:\", mrr_at_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715f3916-22c5-4d10-9426-912bea28e951",
   "metadata": {},
   "source": [
    "a way to fix this error is by fine-tuning the pretrained clip model "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
